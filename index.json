[{"authors":null,"categories":null,"content":"Hi i’m Paul.\nI believe the ideas \u0026amp; methods of the Machine Learning world are so significant that they deserve to be shared beyond the small field of researchers \u0026amp; practitioners that use Ai algorithms on a day to day basis.\nLearning the basic principles of how these algorithms function could have a significant impact on your life.\nFor example, if you are a University student looking at choosing what degree \u0026amp; career to pursue - did you know there is a Machine Learning principle that can help you with that?\nUnfortunately most of these algorithms and most importantly, their underlying principles, are usually hidden by mathematical notation. Whilst I believe that mathematics is one of the most beautiful languages in existence, I also believe that maths does not help in telling the story of how these graceful machine learning concepts work \u0026amp; most importantly - can be of use to you.\nThe aim of this blog will be to use a fundamentally human tool - stories - to explain \u0026amp; teach these machine learning concepts \u0026amp; principles in order to try \u0026amp; spread these ideas to the masses.\n~\nThe first couple of blog posts are going to be old articles that I have had lying around on my hard drive for a couple of years and feel that I need to publish.\nFrom then on, as time permits, we are going to go on a journey exploring the machine learning world together.\nLooking forward to it \u0026amp; \u0026lt;3 from Sydney, Australia\nPaul.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://PaulConyngham.github.io/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"Hi i’m Paul.\nI believe the ideas \u0026amp; methods of the Machine Learning world are so significant that they deserve to be shared beyond the small field of researchers \u0026amp; practitioners that use Ai algorithms on a day to day basis.\nLearning the basic principles of how these algorithms function could have a significant impact on your life.\nFor example, if you are a University student looking at choosing what degree \u0026amp; career to pursue - did you know there is a Machine Learning principle that can help you with that?","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"https://PaulConyngham.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"","date":1550908610,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550908610,"objectID":"34744b3f57eb48dd9d490bbc5678b777","permalink":"https://PaulConyngham.github.io/project/starai-deep-reinforcement-learning-course/","publishdate":"2019-02-23T18:56:50+11:00","relpermalink":"/project/starai-deep-reinforcement-learning-course/","section":"project","summary":"","tags":[],"title":"Starai Deep Reinforcement Learning Course","type":"project"},{"authors":null,"categories":null,"content":"","date":1550908604,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550908604,"objectID":"4485bdf739ae4f7b47cd05a938b3885e","permalink":"https://PaulConyngham.github.io/project/rendering-gym-on-colaboratory/","publishdate":"2019-02-23T18:56:44+11:00","relpermalink":"/project/rendering-gym-on-colaboratory/","section":"project","summary":"","tags":[],"title":"Rendering Gym on Colaboratory","type":"project"},{"authors":null,"categories":null,"content":"","date":1550908596,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550908596,"objectID":"ff02463915d36eba6333e79752a7f59b","permalink":"https://PaulConyngham.github.io/project/pysc2-on-colaboratory/","publishdate":"2019-02-23T18:56:36+11:00","relpermalink":"/project/pysc2-on-colaboratory/","section":"project","summary":"","tags":[],"title":"Pysc2 on Colaboratory","type":"project"},{"authors":null,"categories":[],"content":" Guest starring the Epsilon-Greedy Algorithm, by Paul Steven Conyngham Machine learning people love to give fancy names to things so that no one can understand what they are on about.\nFor someone relatively new to machine learning, \u0026ldquo;The multi armed bandit problem\u0026rdquo; sounds just like one of these fancy names.\nFret not however!\nThe point of this blog post is to explain exactly what a Bandit is and most importantly, why it is usually used as the starting point for anyone looking at learning Reinforcement Learning.\n\u0026nbsp;\n\u0026nbsp;\nPost Overview: In this post I am going to aim to teach you:\n Some core Reinforcement Learning ideas such as the multi-armed bandit, exploration vs. exploitation \u0026amp; the epsilon greedy algorithm. Introduce you to OpenAi gym and why it is important. A programming exercise to help you solidify your understanding of the discussed ideas.  \u0026nbsp;\n\u0026nbsp;\nSo then, what the shell is a bandit?\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nThis.\nA bandit is an old fashioned american name for what we usually call a \u0026ldquo;slot machine\u0026rdquo;.\nHere in Australia we like to call it the \u0026ldquo;pokies\u0026rdquo;.\nGreat, but what the shell is a multi armed bandit?\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nSimply, this:\nA whole bunch of \u0026ldquo;bandits\u0026rdquo; stacked together such there are many \u0026ldquo;arms\u0026rdquo; to pull.\nThis is where the \u0026ldquo;multi armed\u0026rdquo; part comes in.\n\u0026nbsp;\n\u0026nbsp;\nHold up.\nWhy are we talking about slot machines with regards to machine learning?\nWell, one definition for Reinforcement Learning, the subfield of machine learning that we are talking about here deals with:\n\u0026nbsp;\n \u0026ldquo;Finding the optimal strategy to solving a problem in the face of massive uncertainty.\u0026rdquo;\n \u0026nbsp;\nLet\u0026rsquo;s see why defining the multi-armed bandit this way is important by considering an example.\n~\nSay you wanted to drive your car from your home to your work.\nWhen you wake up in the morning, you have no idea how the traffic lights will change or what the other cars will be doing on your way to work.\nYou could encounter 5 cars that delay you at a roundabout, or you could encounter 10 red traffic lights in a row.\nIn order to find this information out, you would have to actually drive the route to work and gather the data.\nSo, not knowing what the traffic will be doing when you wake up in the morning is the uncertainty part in our definition above.\nWe also need a plan of what we will do once we encounter the intersections, roundabouts \u0026amp; traffic lights as we drive our car on our way to work.\nThis plan or strategy is what reinforcement learning aims to figure out.\nEven more specifically, reinforcement learning attempts to learn the optimal strategy - that is to say, the best possible strategy for a specific task - in this case, the best way of driving through all the obstacles of traffic lights, roundabouts etc from home to work.\nThis is the strategy part in our definition above.\n\u0026nbsp;\n\u0026nbsp;\nReinforcement Learning Terminology Decoded #1: In reinforcement learning the name given to the strategy that we are following to solve a given problem is called a policy.\nFollowing the previous example of driving to work. An example of a policy (strategy) would be driving as fast as possible to work.\nAnother example of a policy would be to ignore all red lights (again this could be another strategy).\nOf course your policy could also be something boring. For example, only moving on green traffic lights, stopping for all other cars at roundabouts - like I hope you are doing.\nSummarizing, a policy is the strategy that we use to take incoming information and process it into actions to be taken in the environment.\n\u0026nbsp;\n\u0026nbsp;\nLet us watch a couple of people following the \u0026ldquo;policy\u0026rdquo; of not stopping at red traffic lights.\n\nVideo 1: Here we can see the drivers following the policy of driving through the intersection when observing a red traffic light. :)\nNow we know that we are dealing with finding the best strategies -or policies- in the face of unknown conditions - or uncertainty.\nThe multi armed bandit problem is usually brought up as the starting point of most Reinforcement Learning (RL) text books because it introduces several core RL ideas.\nThe first of which is that in slot machines - you are dealing with uncertainty.\nIn other words - if you go up to a slot machine and pull the lever, you have no idea when you are going to get a cash payout. You also have no idea how much that cash payout might be.\nWe also examine the multi armed bandit as our \u0026ldquo;toy\u0026rdquo; problem for explaining Reinforcement Learning because it teaches us the second core concept with regards to RL.\nThat is, what to do when we have more than one option for solving a problem.\nIn the multi-armed Bandit problem there are many slot machine levers to pull. So - we have many options. Just like you do when driving on your way to work.\nHow then do we decide upon the right strategy for how \u0026amp; when to pull the many different levers of our multi-armed bandit slot machine scenario, or in terms of our machine learning terminology from before - a “policy” for pulling these levers?\nSummarizing, the bandit problem in a nutshell:\nWhat do we do when we have more than one slot machine to choose from and we would like to know which slot machine is going to give us the highest average payout or reward over time. In other words, which slot machine is the best choice?\nLets get cracking on this problem by introducing another Machine Learning idea and your first (baby) Reinforcement Learning Algorithm.\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nExploration vs Exploitation There is this age old problem in life and it goes something like this.\nLet’s say you are on the hunt for a hot date with the eventual goal of picking up a life partner.\nFirst you hop on your favourite dating app of choice.\nAfter chatting to a few people you manage to score yourself a hot date. The way they were coming on to you so strongly on the first date was a was a bit weird, strike out, you decide that\u0026rsquo;s it for the dating app person.\nThe following week you decide dating apps are kind of lame, so you head out to the bar.\nYou manage to work up the courage to approach someone and after talking to them for a bit begin to realise that this person has really bad breath and might not be quite the right person for you.\nFinally, towards the end of the night and after a few drinks, you work up your last bit of courage to go over and talk to an attractive person, who has been looking at you over their shoulder all night.\nYou hit it off and get their number. Two years later you are in a happy relationship and decide to get married, happily, ever after.\nWhat was just described above is an age old problem in machine learning.\nHow much time do you spend \u0026ldquo;exploring\u0026rdquo; - going on dates with people looking for a partner etc,\nVersus,\nHow much time do you spend, ahem, \u0026ldquo;exploiting\u0026rdquo; - being in a relationship with someone etc.\n\u0026nbsp;\nLets make the topic of exploration vs exploitation more concrete with a few more examples.\nAnother example of exploration versus exploitation is how much time you spend looking at potential job opportunities (exploring) vs being in a particular job (exploiting).\nYet another example would be if you are a stock trader. How much time do you spend searching for the best trading strategy (exploring) vs implementing a strategy on the stock market (exploiting).\nThe reason we consider the exploration vs exploitation idea on the multi armed bandit problem, is that remember there is more than one machine and each slot machine\u0026rsquo;s is tuned slightly differently, such that each slot machine will give different average cash payouts.\nHow then do we go about discovering which Bandit slot machine pays out the most?\nIf you have not guessed it already, what we would like to do is spend some time exploring - to see amongst our many slot machines, which slot machine gives the best average payout.\nWhen we have discovered which machine gives the best average payout, we then want to keep exploiting this machine.\nOk then. Time for your first RL (baby) algorithm\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;z\nThe Epsilon-Greedy Algorithm Simply, the Episilon Greedy Algorithm is this:\nSeriously though, if you did not understand that no dramas at all.\nThe rest of this post is going to be about breaking apart the mathematical notation above into a more human readable format :).\nLet\u0026rsquo;s do it.\n\u0026nbsp;\n\u0026nbsp;\nReinforcement Learning Terminology Decoded #2: In machine learning we use another name for the bell curve (pictured below). That name is the \u0026ldquo;normal distribution\u0026rdquo;. This is technically more correct as we will see soon.\nKey idea: \u0026ldquo;A normal distribution is \u0026ldquo;centered\u0026rdquo; around an average number.\u0026rdquo;\nFigure 2: Normal distribution centered around 100.\n\u0026nbsp;\n\u0026nbsp;\nSay we wanted to implement some kind of Exploration vs. Exploitation on a problem.\nHow would we do it?\nLet\u0026rsquo;s examine the multi-armed bandit problem in a little bit more detail, to explain what epsilon \u0026amp; greedy is and why we are examining the multi armed bandit problem in the first place.\nSay we have a bandit slot machine and that if I pulled the lever on the bandit 300 times, I would get data of the payout of that slot machine that looked like a bell curve like so:\nFigure 1. Average payout of a slot machine based on the amount of times that the lever has been pulled. For example, we can see that at around the $70 mark the lever on the slot machine has been pulled around 44 times.\nA slot machine is meant to be random - and we would like to discover a pattern in the “random” data if it exists.\nWe use the idea of a distribution to represent our Bandit\u0026rsquo;s distribution of possible cash payouts. In essence the distribution is representing uncertainty.\nEvery time we pull our lever on the bandit, it will give a different cash payout.\nIn figure 1, sometimes it will be 65 dollars, sometimes it will be 80, but the average payout over time will be $70.\nThe middle of the bell graph in figure 1 is centered around the 70 dollar mark. We can therefore say that our slot machine in the graph in figure 1 has an average payout of around 70 dollars.\nLet us now examine the case of two slot machines.\nEach slot machine will each give payouts according to two different normal distributions.\nWhat does this mean? Have a look at the image below:\nFigure 3, Here we can see that our first bandit, the bandit in blue, which had an average value of 70 - is centered around the 70 dollar mark.\nThe bandit in pink in bandit #2.\nWe sample Bandit #2 by pulling the lever on Bandit #2 over and over and collect new data on how Bandit #2 performs, seen in figure 3.\nWhen plotted, we can see that bandit #2 has a different distribution and has a average payout centred around the $65 mark.\nWe have now introduced two bandits, bandit #1 \u0026amp; bandit #2.\nWe have also learned that each bandit has a different distribution.\nWhy is this important?\nBy “sampling” each bandit and building distributions, we were able to determine which one of our bandits, on average, paid out the most money.\nThe answer, if you have not guessed it already, is that Bandit #1 wins - with an average payout of 70 dollars as opposed to bandit #2, only paying out on average 65 dollars.\nLinking back to what we talked about earlier, what we were doing by sampling each bandit and building distributions was doing the exploration phase of the Epsilon Greedy algorithm.\nIn summary,\nGiven a problem with many options.\nWhat we would like to do is explore all the options available to us by randomly sampling between the different options, over and over until we start to build up a distribution of data for each option.\nSo, gather data on option 1 (Bandit #1), then a little bit of data on option 2 (Bandit #2) over and over until we have built distributions for all of our options.\nThen, we calculate the average seperately for each individual distribution we have gathered to discover the best option.\nOnce we have discovered the best option we can go \u0026ldquo;greedy\u0026rdquo; and continue to exploit it.\nLet\u0026rsquo;s now see where Epsilon comes in.\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nEpsilon Reinforcement Learning Terminology Decoded #3: According to wikipedia Epsilon is the 5th letter of the Greek alphabet and looks like this: \u0026nbsp;\nε \u0026nbsp;\nHieroglyphics right? Epsilon is a greek letter. But what epsilon is used for is the interesting bit.\n\u0026nbsp;\nEpsilon-greedy is a mechanism used to decide which option to exploit.\nWhen sampling Epsilon controls the ratio between the amount of time we spend exploring vs how much time we spend exploiting.\nThink of epsilon as a volume knob, which you can turn that controls the amount of exploration you do versus the amount of exploitation.\nFigure 4, Epsilon can be thought of a volume knob. Initially epsilon is high and exploration is maximised with a value of 10, but as time progresses we \u0026ldquo;turn\u0026rdquo; the volume knob down until it is low and exploitation is maxised having a value of 1. In the illustration above we are scaling from 10 to 1. In practise we usually scale from 1 to 0\nInitially we want to explore as much as possible to discover all the options available to us. To do this we set Epsilon to one.\nSo when ε = 1, exploration is maximised.\nand when ε = 0, exploitation is maximised.\nHow then do we go from an epsilon of 1 down to 0?\nWell, one way to do it is to choose a mathematical function to control Epsilon.\nThere are many mathematical functions you can use to control Epsilon. However in this example we are going control Epsilon using a linear function\u0026hellip;or more commonly known as a straight line.\nThink of our linear function as the volume knob controlling the ratio between exploration and exploitation.\n\u0026nbsp;\nHere we derive a simple linear way of controlling Epsilon. Skip this part if you do not care about the maths. \u0026nbsp;\nFrom high school mathematics, a straight line has the form:\n(1.) $y = mx + c ​$\nwhere m equals the gradient of the line, and c is the Y axis intercept.\nFigure 5, A straight line plotted. Here the variable C (the Y axis intercept) has been set to zero.\nIf we choose a straight line as our function for controlling Epsilon then our function f(x) becomes:\n(2.) $f(x) = mx + c$\nWe know we would like to start off with exploration maximised, so equal to one. We would also like to scale down Epsilon over time. One way to control Epsilon would then be to subtract a straight line from one. Epsilon can then be defined as:\n(3.) $ε = 1 - f(x)​$\nSubstituing equation 2 yields:\n(4.) $ε = 1 - (mx + c) $\nIf we replace x for t (time) this gives us our final equation:\n(5.) $ε = 1 - mt - c ​$\nFinally, if we set c to zero, we get the final diagram below. Exactly the type of operation that we are after- scaling epsilon from 1 to 0:\n(6.) $ε = 1 - mt $\nFigure 6, Epsilon decreasing linearly over time.\n\u0026nbsp;\nSo to summarize the epsilon-greedy process, via sampling we slowly start to figure out what is the best possible option to use to solve a problem. Simultaneously, whilst we are sampling we turn down the Epsilon volume knob.\nTurning down Epsilon decreases the amount of exploration we are doing and starts focusing in on the best solution we have found so far to solve a given problem. Aka going \u0026ldquo;greedy\u0026rdquo;.\nTaking this back to our toy problem of bandits, we first start exploring by pulling different levers at random between Bandit # 1 and Bandit # 2.\nFollowing this random sampling process, we start to build distributions of our different bandit options and see which bandit is going to give us the highest average reward in the form of cash payout.\nSimultanously as we are sampling, we start to decrease Epsilon and start focusing on the bandit option that is going to give us the highest average cash reward.\nIn the video below I run though a visual example of the Epsilon-Greedy algorithm running in practise.\n\nVideo 2: pulling it all together, lets run the epsilon greedy algorithm\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nThe final piece of the puzzle: the Reward-Averaging Sampling learning rule. \u0026nbsp;\nIn reinforcement learning we like to refer to our algorithm systems as \u0026ldquo;agents\u0026rdquo;. So far we have learned about the \u0026ldquo;Epsilon-Greedy\u0026rdquo; agent.\nWe have seen that it is a synthesis of a purely exploratory agent and a completely greedy agent.\nIn the multiarmed bandit slot machine problem a purely exploratory agent would sample all the different bandits options available evenly - building a distribution for each bandit.\nHowever this has the downside that the agent never gets to use its knowledge of the best option it has discovered so far.\nOne way to think of a purely exploratory agent is a student that goes to University and starts 5 different degrees - but quits after 1 semester in all of them - never sticking it out to see which degree might have been best for them, or in other words, giving them the highest average reward. ;)\nHowever, A purely greedy agent would choose a bandit and stick to it\u0026rsquo;s choice for all eternity.\nA purely greedy agent can be though of as being very \u0026ldquo;narrow minded\u0026rdquo; as it will not try other bandit options to see if they provide better average long term reward.\nTo get the best of both worlds, the Epsilon-greedy agent is designed to explore at an Epsilon chance whilst the rest of the time it goes greedy on the best option it had discovered so far.\nIn our example, we have seen that we are able to scale the Epsilon chance using a linear function to control the amount of time we spend exploring versus the amount of time we spend going greedy.\nThe idea here is that the Greedy mechanism helps the agent exploit the best option it has discovered so far, whilst the small amount of exploration leftover ensures that our agent keeps searching the other options available - to prove that there are not even better options out there.\nThe last remaining piece of the puzzle in the Epsilon-Greedy algorithm is how do we assign the idea of \u0026ldquo;value\u0026rdquo; to each of our different bandits options.\n\u0026nbsp;\n\u0026nbsp;\nReinforcement Learning Terminology Decoded #4: Q Values Say we are at a single slot machine. We pull the slot machine\u0026rsquo;s lever, and the shapes on the slot machines screen begin to whur! After a small amount of time the characters stop. We see \u0026ldquo;apple apple pear\u0026rdquo;. Unfortunately, no money comes out of the machine.\nIn RL, we refer to the information on the slot machine\u0026rsquo;s screen - \u0026ldquo;apple apple pear\u0026rdquo;- as State.\nThe Action we took was pulling the lever.\nThe assignment of reward to this State-Action pair combo is what we call a Q-Value. Q in this case just stands for Quality.\nIn our example we take an action and pull the lever on the slot machine. The information we get back as a result of this action is \u0026ldquo;apple apple pear\u0026rdquo; and we refer to this information as State. The value of this State-Action pair is the resulting reward. So in this case our cash payout was zero. Therefore our Q value for this State-Action pair is 0.\nThe mathematical notation for assignment of a Q value looks like this:\n$Q(s,a) = 0$\nWhere s \u0026amp; a represent state \u0026amp; action respectively.\nOn a final note the definition of Q values above is not quite the entire story, but we will get to why that is soon.\n\u0026nbsp;\n\u0026nbsp;\nLet\u0026rsquo;s say we pull the lever on one of our slot machine systems. We can assign a name to this action- lets call it A1.\nThe result from our action- A1- is that we see new information from the slot machine system and encounter a reward - which we can call R1.\nGiven some state of the multi-slot machine bandit system we can specify a mathematical function to define the long term average reward. In Reinforcement Learning, we also like to call the average in this scenario \u0026ldquo;the expected value\u0026rdquo;.\nGiven many actions, or pulls of our lever on the bandit, the expected Q value or average over time for any one bandit choice, can be given by:\n\u0026nbsp;\n$Q_{n}(a) = \\frac{1}{n}(r1 + r2 +r3) .(1) $ \u0026nbsp;\nWhere:\n Q is our Q Value standing for Quality, this is an arbitary naming convention n stands for the number of times that bandit (or state) has been visited r is reward from visiting a bandit, n many times  \u0026nbsp;\nSo for example, say we sample bandit #1, 3 times.\nThis means that we get three different rewards r1, r2 \u0026amp; r3.\nBecause we visited bandit#1 3 times we also know that n = 3\nTherefore:\n$Q = (1/n)*(r1+r2+r3) .(2)$ \u0026nbsp;\nsubstituting 3 for n gives us:\n\u0026nbsp;\n$Q = (1\u0026frasl;3)*(r1+r2+r3) .(3)$ \u0026nbsp;\nEquation 3: the total expected reward or average, for a bandit option that has been \u0026ldquo;pulled\u0026rdquo; three times.\n\u0026nbsp;\nBy pulling the lever, we are sampling our Bandit # 1, 3 times. As a result, we get 3 different rewards. We calculate the average reward for Bandit #1 and this gives us our Q value associated with that Bandit. We now know the \u0026ldquo;Quallity\u0026rdquo; of Bandit #1. Remember in machine learning in general we like to call the average the \u0026ldquo;total expected reward.\u0026rdquo;\nWe just worked out the average or expected value, for Bandit #1.\nNow lets say we wanted to keep track of the total expected reward of another bandit, say Bandit #2. In order to work out the average reward for bandit #2, we will have to keep track of all of Bandit 2\u0026rsquo;s reward variables \u0026amp; number of times it has been visited in a seperate table to those of bandit #1.\nLike so:\nTable 1: Each Bandit has a seperate associated Q value, which we update individually each time that bandit is sampled.\n\u0026nbsp;\nIn a real life multi armed bandit scenario, we would have to keep track of many reward variables the longer our agent is run.\nFor 8 \u0026ldquo;samples\u0026rdquo; of our bandit this would looks like this:\n$ Q = (1\u0026frasl;8)*(r1+r2+r3+r4+r5+r6+r7+r8) \u0026hellip;$\nIf we were to continue to run this algorithm for a while, you can quickly see that we would run out of computer memory.\nIn order to reduce the amount of memory that is in use, we can use some mathematical trickery to compress the above equation, such that when we update our Q value with every visit to the bandit our formula becomes:\n$ Q{n+1}(a) = Q{n}(a) + \\frac{1}{n+1}(r{n+1}- Q{n}(a)) .(4) $ Where:\n $ Q_{n+1}(a) $ is the update to our Q table\u0026rsquo;s value for a specific bandit, say bandit #1 $ Q_{n}(a) $ is the current value in our Q table for a specific bandit n +1 is the number of times that bandit (or state) has been visited. There is a \u0026ldquo;+1\u0026rdquo; here to stop a divide by zero error at the intialization of the algorithm. r+1 is the updated reward from out latest sample of the bandit.  Finally, now that we have calculated our table of Q values, we need to select from our table the Bandit with the highest average payout.\nAgain, we are selecting the Bandit with the highest average payout as our \u0026ldquo;choice\u0026rdquo; amoungst the many options of Bandits that are available to us because it is the best choice we have discovered so far.\nWe do this by something that sounds like a super hero - using the argmax function.\nArgmax stands for maximum argument and simpy put, means that given a choice of a whole bunch of numbers, say 1, 2, 3 - to choose the biggest number in the set, in this case the argmax of our set is 3.\nIf we were to take the argmax of our table 1 from before \u0026amp; substituting Q1=1, Q2=2 \u0026amp; Q3=3, then taking the argmax would just be the selection of the highest value in the table, the red square, like so:\n\u0026nbsp;\n\u0026nbsp;\nIf we continue to run our Epsilon Greedy agent for a while on a problem, say the multiarmed bandit, as discussed before the values of our bandit options in our Q table change. We then retake the argmax, and viola! once again find the biggest value in our set.\nThe Reward-Averaging Sampling learning rule is then the heart of the epsilon greedy algorithm, or the forumula we use to update our Q table, you have to use this code in the upcoming exercise to make the Epsilon-Greedy agent work.\nIn summary, Epsilon Greedy is a powerful method for when you have a whole bunch of options, whose reward distribution you do not know \u0026amp; are able to find the optimal reward distribution using sampling.\nThe important thing to take away is that you can replace the Bandit problem with any problem which has different options which have hidden distributions. For example - an email marketing campaign, and applying the exact same methods, determine the best products to display in that email campaign.\n\u0026nbsp;\n\u0026nbsp;\n\u0026nbsp;\nEnter OpenAi Gym.  \u0026ldquo;What I cannot create, I do not understand\u0026rdquo; - Richard Feynman.\n We are going to make extensive use of Gym in accompanying programming exercise to this blog post. I highly suggest you have a crack at implementing Epsilon-Greedy on a multi-armed bandit scenario, as whilst I hope you may have gained the intuitiion of how the Multi Armed Bandit problem \u0026amp; Epsilon-Greedy Algorithm works in the explanation above - the best way to understand it is to build it yourself.\nOnly 3 years ago, OpenAi released Gym. There was little to no media coverage of the significance of this release.\nIt was actually really important.\nBefore OpenAi gym, there was no “standard” framework you could use test your Reinforcement learning algorithms on.\nIf a researcher in China was doing some work on Reinforcement Learning \u0026amp; wanted to have a benchmark to test it against a researcher\u0026rsquo;s work in Australia - this was not possible, as both researchers in their respective countries were using different frameworks that they had written themselves to test their algorithms on.\nIn the accompanying exercise to this post below, we are going to make extensive use of Gym and implement an epsilon-greedy algorithm to solve the bandit problem.\nLets get cracking building your very first intro reinforcement learning algorithm with OpenAi gym.\nWe will be using Google Colaboratory for the compute, so you will also need a Google account.\nTo start the exercise click the link here, or image below:\n\u0026nbsp;\n\n","date":1550827749,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550827749,"objectID":"bb0b0984b7de08fc38999f763b446858","permalink":"https://PaulConyngham.github.io/post/what-the-shell-is-a-multi-armed-bandit-guest-starring-the-epsilon-greedy-algorithm/","publishdate":"2019-02-22T20:29:09+11:00","relpermalink":"/post/what-the-shell-is-a-multi-armed-bandit-guest-starring-the-epsilon-greedy-algorithm/","section":"post","summary":"Guest starring the Epsilon-Greedy Algorithm, by Paul Steven Conyngham Machine learning people love to give fancy names to things so that no one can understand what they are on about.\nFor someone relatively new to machine learning, \u0026ldquo;The multi armed bandit problem\u0026rdquo; sounds just like one of these fancy names.\nFret not however!\nThe point of this blog post is to explain exactly what a Bandit is and most importantly, why it is usually used as the starting point for anyone looking at learning Reinforcement Learning.","tags":[],"title":"What the Shell is a Multi Armed Bandit? - An introduction to Reinforcement Learning","type":"post"},{"authors":null,"categories":[],"content":"","date":1548585782,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548585782,"objectID":"b128b7694e4de962e0088bcb1bcb254f","permalink":"https://PaulConyngham.github.io/test/","publishdate":"2019-01-27T21:43:02+11:00","relpermalink":"/test/","section":"","summary":"","tags":[],"title":"Test","type":"page"}]